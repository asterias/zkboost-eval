{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f620383",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Datasets:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Dataset: breast_cancer ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Datasets:  50%|█████     | 1/2 [00:07<00:07,  7.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Terms Depth Trees | Simple Acc Fixed Acc Wider Acc XGB Acc | Simple T  Fixed T  Wider T  XGB T\n",
      "----------------------------------------------------------------------------------------------\n",
      "10     4      50    | 0.9561    0.9474    0.9123    0.9649   | 2.44      2.76      1.93      0.04  \n",
      "\n",
      "=== Dataset: credit_card_default ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Datasets: 100%|██████████| 2/2 [00:53<00:00, 26.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Terms Depth Trees | Simple Acc Fixed Acc Wider Acc XGB Acc | Simple T  Fixed T  Wider T  XGB T\n",
      "----------------------------------------------------------------------------------------------\n",
      "10     4      50    | 0.8229    0.7677    0.7627    0.8242   | 7.67      16.84     15.79     0.51  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np, math, time\n",
    "import matplotlib.pyplot as plt          # (kept; not used in benchmark)\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import xgboost as xgb\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ──────────────────────────────────────────────────────────\n",
    "# Fixed-point helpers\n",
    "# ──────────────────────────────────────────────────────────\n",
    "SCALE = 100_000          # 5-digit precision\n",
    "INV_SCALE = 1 / SCALE\n",
    "\n",
    "def float_to_fixed(x):     return int(round(x * SCALE))\n",
    "def fixed_to_float(xf):    return xf * INV_SCALE\n",
    "def fixed_mul(xf, yf):     return (xf * yf) // SCALE\n",
    "def fixed_div(xf, yf):     return (xf * SCALE) // yf if yf else 0\n",
    "\n",
    "# integer exp / logistic (for reference)\n",
    "def fixed_exp(xf, terms=10):\n",
    "    res = SCALE\n",
    "    term = SCALE\n",
    "    for i in range(1, terms + 1):\n",
    "        term = fixed_mul(term, fixed_div(xf, float_to_fixed(i)))\n",
    "        res += term\n",
    "    return res\n",
    "\n",
    "def fixed_sigmoid(xf, terms=10):\n",
    "    lo, hi = float_to_fixed(-5), float_to_fixed(5)\n",
    "    xf = max(lo, min(hi, xf))\n",
    "    if xf >= 0:\n",
    "        e = fixed_exp(-xf, terms)\n",
    "        return fixed_div(SCALE, SCALE + e)\n",
    "    else:\n",
    "        e = fixed_exp(xf, terms)\n",
    "        return fixed_div(e, SCALE + e)\n",
    "\n",
    "# stable float sigmoid\n",
    "def stable_sigmoid(x):\n",
    "    x = np.asarray(x)\n",
    "    out = np.empty_like(x)\n",
    "    pos = x >= 0\n",
    "    out[pos]  = 1 / (1 + np.exp(-x[pos]))\n",
    "    expx      = np.exp(x[~pos])\n",
    "    out[~pos] = expx / (1 + expx)\n",
    "    return out\n",
    "\n",
    "# ──────────────────────────────────────────────────────────\n",
    "# Integer sigmoid variants\n",
    "# ──────────────────────────────────────────────────────────\n",
    "# LUT-based (your original)\n",
    "SIGMOID_LUT = {}\n",
    "SIGMOID_LUT_RESOLUTION = 100  # 0.001 granularity (vs 0.01 before)\n",
    "SIGMOID_LUT_CLAMP_MIN = -500_000\n",
    "SIGMOID_LUT_CLAMP_MAX = 500_000\n",
    "\n",
    "def init_sigmoid_lut():\n",
    "    for xf in range(SIGMOID_LUT_CLAMP_MIN, SIGMOID_LUT_CLAMP_MAX + 1, SIGMOID_LUT_RESOLUTION):\n",
    "        x = xf / SCALE\n",
    "        SIGMOID_LUT[xf] = int(round((1 / (1 + math.exp(-x))) * SCALE))\n",
    "\n",
    "def lut_fixed_sigmoid(xf):\n",
    "    xf = max(SIGMOID_LUT_CLAMP_MIN, min(SIGMOID_LUT_CLAMP_MAX, xf))\n",
    "    base = SIGMOID_LUT_RESOLUTION * (xf // SIGMOID_LUT_RESOLUTION)\n",
    "    rem = xf - base\n",
    "    s0 = SIGMOID_LUT.get(base, SCALE // 2)\n",
    "    s1 = SIGMOID_LUT.get(base + SIGMOID_LUT_RESOLUTION, SCALE // 2)\n",
    "    return s0 + ((s1 - s0) * rem) // SIGMOID_LUT_RESOLUTION\n",
    "\n",
    "# Call once before training\n",
    "init_sigmoid_lut()\n",
    "\n",
    "\n",
    "# NEW wider piece-wise: linear on [-2,2], saturates outside\n",
    "def wider_piecewise_sigmoid(xf: int) -> int:\n",
    "    \"\"\"Improved piecewise: linear in [-2, 2], smoothly clipped\"\"\"\n",
    "    two = 2 * SCALE\n",
    "    if xf <= -two: return 0\n",
    "    if xf >= two: return SCALE\n",
    "    return SCALE * (xf + two) // (4 * SCALE)\n",
    "\n",
    "# ──────────────────────────────────────────────────────────\n",
    "# Histogram-tree used by all boosted models (unchanged)\n",
    "# ──────────────────────────────────────────────────────────\n",
    "class XGBoostTreeClassifier:\n",
    "    def __init__(self, max_depth=3, min_samples_split=10,\n",
    "                 lambda_=1, gamma=0, colsample_bytree=1.0, num_bins=64):\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.lambda_  = lambda_\n",
    "        self.gamma    = gamma\n",
    "        self.colsample_bytree = colsample_bytree\n",
    "        self.num_bins = num_bins\n",
    "        self.tree     = None\n",
    "\n",
    "    # --- recursive training ---\n",
    "    def fit(self, X, grad, hess, depth=0):\n",
    "        if depth >= self.max_depth or len(X) < self.min_samples_split:\n",
    "            return np.clip(np.sum(grad) / (np.sum(hess) + self.lambda_), -10, 10)\n",
    "\n",
    "        n_features = X.shape[1]\n",
    "        feats = np.random.choice(n_features,\n",
    "                                 max(1, int(self.colsample_bytree*n_features)),\n",
    "                                 replace=False)\n",
    "        best_gain = -np.inf\n",
    "        best_feat, best_split = None, None\n",
    "\n",
    "        for f in feats:\n",
    "            col = X[:,f]\n",
    "            if col.min()==col.max(): continue\n",
    "            bins = np.linspace(col.min(), col.max(), self.num_bins+1)\n",
    "            ids  = np.digitize(col, bins[:-1], right=False)\n",
    "\n",
    "            G = np.bincount(ids, weights=grad, minlength=self.num_bins+2)\n",
    "            H = np.bincount(ids, weights=hess, minlength=self.num_bins+2)\n",
    "            G_L=H_L=0.0\n",
    "            Gtot,Htot = G.sum(), H.sum()\n",
    "\n",
    "            for b in range(1,self.num_bins):\n",
    "                G_L += G[b];  H_L += H[b]\n",
    "                G_R, H_R = Gtot-G_L, Htot-H_L\n",
    "                if H_L==0 or H_R==0: continue\n",
    "                gain = 0.5*((G_L**2)/(H_L+self.lambda_) +\n",
    "                             (G_R**2)/(H_R+self.lambda_)) \\\n",
    "                       -0.5*(Gtot**2)/(Htot+self.lambda_)\n",
    "                if gain > best_gain and gain > self.gamma:\n",
    "                    best_gain, best_feat, best_split = gain, f, bins[b]\n",
    "\n",
    "        if best_feat is None:\n",
    "            return np.clip(np.sum(grad)/(np.sum(hess)+self.lambda_), -10,10)\n",
    "\n",
    "        left  = X[:,best_feat] <= best_split\n",
    "        right = ~left\n",
    "        ltree = self.fit(X[left],  grad[left],  hess[left],  depth+1)\n",
    "        rtree = self.fit(X[right], grad[right], hess[right], depth+1)\n",
    "        return (best_feat, best_split, ltree, rtree)\n",
    "\n",
    "    # --- inference ---\n",
    "    def _pred_row(self, x, node):\n",
    "        if not isinstance(node, tuple): return node\n",
    "        f, sp, l, r = node\n",
    "        return self._pred_row(x, l) if x[f] <= sp else self._pred_row(x, r)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.array([self._pred_row(x, self.tree) for x in X])\n",
    "\n",
    "# ──────────────────────────────────────────────────────────\n",
    "# Float baseline (unchanged)\n",
    "# ──────────────────────────────────────────────────────────\n",
    "class SimpleXGBoostClassifier:\n",
    "    def __init__(self, n_estimators=50, max_depth=3, learning_rate=0.1,\n",
    "                 lambda_=1, gamma=0, subsample=0.8, colsample_bytree=0.8):\n",
    "        self.n_estimators=n_estimators; self.max_depth=max_depth\n",
    "        self.learning_rate=learning_rate; self.lambda_=lambda_\n",
    "        self.gamma=gamma; self.subsample=subsample\n",
    "        self.colsample_bytree=colsample_bytree\n",
    "        self.trees=[]; self.initial_logit=0\n",
    "\n",
    "    def sigmoid(self,x): return stable_sigmoid(x)\n",
    "\n",
    "    def fit(self,X,y):\n",
    "        eps=1e-6\n",
    "        p=y.mean(); self.initial_logit=np.log(p/(1-p))\n",
    "        y_pred=np.full_like(y,self.initial_logit,dtype=float)\n",
    "        for _ in range(self.n_estimators):\n",
    "            idx=np.random.choice(len(X),int(len(X)*self.subsample),replace=False)\n",
    "            p=self.sigmoid(y_pred[idx]); p=np.clip(p,eps,1-eps)\n",
    "            grad=p-y[idx]; hess=p*(1-p)\n",
    "            tree=XGBoostTreeClassifier(max_depth=self.max_depth,\n",
    "                                       lambda_=self.lambda_,gamma=self.gamma,\n",
    "                                       colsample_bytree=self.colsample_bytree)\n",
    "            tree.tree=tree.fit(X[idx],grad,hess)\n",
    "            y_pred-=self.learning_rate*tree.predict(X)\n",
    "            self.trees.append(tree)\n",
    "\n",
    "    def predict_proba(self,X):\n",
    "        y_pred=np.full(X.shape[0],self.initial_logit)\n",
    "        for t in self.trees: y_pred-=self.learning_rate*t.predict(X)\n",
    "        return self.sigmoid(y_pred)\n",
    "\n",
    "    def predict(self,X): return (self.predict_proba(X)>=0.5).astype(int)\n",
    "\n",
    "# ──────────────────────────────────────────────────────────\n",
    "# Generic fixed-point booster (plug-in integer sigmoid)\n",
    "# ──────────────────────────────────────────────────────────\n",
    "class GenericFixedPointXGB(SimpleXGBoostClassifier):\n",
    "    def __init__(self, int_sigmoid, terms=10, **kw):\n",
    "        super().__init__(**kw)\n",
    "        self.int_sigmoid=int_sigmoid\n",
    "        self.terms=terms\n",
    "        self.scaled_lr=float_to_fixed(self.learning_rate)\n",
    "\n",
    "    # vectorised integer sigmoid\n",
    "    def sigmoid(self,x):\n",
    "        return np.fromiter((self.int_sigmoid(xi) for xi in x),\n",
    "                           dtype=int,count=len(x))\n",
    "\n",
    "    def fit(self,X,y):\n",
    "        y_fix=np.array([float_to_fixed(v) for v in y],dtype=int)\n",
    "        m=y_fix.mean(); self.initial_logit=float_to_fixed(math.log(m/(SCALE-m)))\n",
    "        y_pred=np.full_like(y_fix,self.initial_logit)\n",
    "\n",
    "        for _ in range(self.n_estimators):\n",
    "            idx=np.random.choice(len(X),int(len(X)*self.subsample),replace=False)\n",
    "            p_hat=self.sigmoid(y_pred[idx])\n",
    "            grad=p_hat-y_fix[idx]\n",
    "            hess=(p_hat*(SCALE-p_hat))//SCALE\n",
    "            tree=XGBoostTreeClassifier(max_depth=self.max_depth,\n",
    "                                       lambda_=self.lambda_,gamma=self.gamma,\n",
    "                                       colsample_bytree=self.colsample_bytree)\n",
    "            tree.tree=tree.fit(X[idx],grad,hess)\n",
    "            upd=tree.predict(X)\n",
    "            for i,u in enumerate(upd):\n",
    "                y_pred[i]-=fixed_mul(self.scaled_lr,float_to_fixed(u))\n",
    "            self.trees.append(tree)\n",
    "\n",
    "    def predict_proba(self,X):\n",
    "        y_pred=np.full(X.shape[0],self.initial_logit)\n",
    "        for t in self.trees:\n",
    "            upd=t.predict(X)\n",
    "            for i,u in enumerate(upd):\n",
    "                y_pred[i]-=fixed_mul(self.scaled_lr,float_to_fixed(u))\n",
    "        return self.sigmoid(y_pred)\n",
    "\n",
    "    def predict(self,X): return (self.predict_proba(X)>=SCALE//2).astype(int)\n",
    "\n",
    "# wrappers for convenience\n",
    "FixedLUT   = lambda **kw: GenericFixedPointXGB(lut_fixed_sigmoid,   **kw)\n",
    "FixedWider = lambda **kw: GenericFixedPointXGB(wider_piecewise_sigmoid, **kw)\n",
    "\n",
    "# ──────────────────────────────────────────────────────────\n",
    "# Integer feature binning (unchanged)\n",
    "# ──────────────────────────────────────────────────────────\n",
    "def bin_features_fixed(X,num_bins=64):\n",
    "    Xb=np.zeros_like(X,dtype=np.int32); s=num_bins-1\n",
    "    for j in range(X.shape[1]):\n",
    "        c=X[:,j]; mn, mx=c.min(),c.max()\n",
    "        if mn==mx: continue\n",
    "        cint=np.round(c*SCALE).astype(np.int64)\n",
    "        bins=((cint-round(mn*SCALE))*s)//(round(mx*SCALE)-round(mn*SCALE))\n",
    "        Xb[:,j]=np.clip(bins,0,s)\n",
    "    return Xb\n",
    "\n",
    "# ──────────────────────────────────────────────────────────\n",
    "# Dataset helpers\n",
    "# ──────────────────────────────────────────────────────────\n",
    "def load_credit_dataset():\n",
    "    d=np.genfromtxt('credit_default.csv',delimiter=',',skip_header=1,filling_values=0)\n",
    "    X=d[:,1:-1]; y=(d[:,-1]>0.5).astype(int)\n",
    "    return X,y\n",
    "\n",
    "datasets={\n",
    "    \"breast_cancer\":      load_breast_cancer(return_X_y=True),\n",
    "    \"credit_card_default\":load_credit_dataset()\n",
    "}\n",
    "\n",
    "# ──────────────────────────────────────────────────────────\n",
    "# Benchmark grid\n",
    "# ──────────────────────────────────────────────────────────\n",
    "depths  = [4]\n",
    "trees_l = [50]\n",
    "runs    = 1\n",
    "terms   = 10   # kept for completeness\n",
    "\n",
    "for ds,(X,y) in tqdm(datasets.items(),desc=\"Datasets\",leave=True):\n",
    "    print(f\"\\n=== Dataset: {ds} ===\")\n",
    "    Xf=StandardScaler().fit_transform(X)\n",
    "    results=[]\n",
    "\n",
    "    for d in depths:\n",
    "        for nt in trees_l:\n",
    "            sm_acc=sm_t=[]; fx_acc=fx_t=[]; wd_acc=wd_t=[]; xb_acc=xb_t=[]\n",
    "            sm_acc,fx_acc,wd_acc,xb_acc=[],[],[],[]\n",
    "            sm_t,fx_t,wd_t,xb_t=[],[],[],[]\n",
    "\n",
    "            for r in range(runs):\n",
    "                Xtr,Xte,ytr,yte=train_test_split(Xf,y,test_size=0.2,\n",
    "                                                 random_state=42+r)\n",
    "\n",
    "                # Simple float\n",
    "                s=SimpleXGBoostClassifier(n_estimators=nt,max_depth=d)\n",
    "                t0=time.time(); s.fit(Xtr,ytr); sm_t.append(time.time()-t0)\n",
    "                sm_acc.append((s.predict(Xte)==yte).mean())\n",
    "\n",
    "                # Binning for fixed models\n",
    "                Xtrb,Xteb=bin_features_fixed(Xtr),bin_features_fixed(Xte)\n",
    "\n",
    "                # LUT fixed\n",
    "                f=FixedLUT(n_estimators=nt,max_depth=d)\n",
    "                t0=time.time(); f.fit(Xtrb,ytr); fx_t.append(time.time()-t0)\n",
    "                fx_acc.append((f.predict(Xteb)==yte).mean())\n",
    "\n",
    "                # Wider fixed\n",
    "                w=FixedWider(n_estimators=nt,max_depth=d)\n",
    "                t0=time.time(); w.fit(Xtrb,ytr); wd_t.append(time.time()-t0)\n",
    "                wd_acc.append((w.predict(Xteb)==yte).mean())\n",
    "\n",
    "                # Real xgboost\n",
    "                xg=xgb.XGBClassifier(n_estimators=nt,max_depth=d,\n",
    "                                     verbosity=0,eval_metric='logloss')\n",
    "                t0=time.time(); xg.fit(Xtr,ytr); xb_t.append(time.time()-t0)\n",
    "                xb_acc.append((xg.predict(Xte)==yte).mean())\n",
    "\n",
    "            results.append((terms,d,nt,\n",
    "                            np.mean(sm_acc),np.mean(fx_acc),np.mean(wd_acc),np.mean(xb_acc),\n",
    "                            np.mean(sm_t), np.mean(fx_t), np.mean(wd_t), np.mean(xb_t)))\n",
    "\n",
    "    # print table\n",
    "    hdr=(\"Terms Depth Trees | Simple Acc Fixed Acc Wider Acc XGB Acc | \"\n",
    "         \"Simple T  Fixed T  Wider T  XGB T\")\n",
    "    print(hdr); print(\"-\"*len(hdr))\n",
    "    for r in results:\n",
    "        print(f\"{r[0]:<5}  {r[1]:<5}  {r[2]:<5} | \"\n",
    "              f\"{r[3]:<9.4f} {r[4]:<9.4f} {r[5]:<9.4f} {r[6]:<8.4f} | \"\n",
    "              f\"{r[7]:<9.2f} {r[8]:<9.2f} {r[9]:<9.2f} {r[10]:<6.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2aba3da4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Datasets:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Dataset: breast_cancer ===\n",
      "4      50     | Simple        Fixed-LUT     Fixed-Wider   Fixed-Taylor  XGB          \n",
      "----------------------------------------------------------------------------------------------------\n",
      "              | 0.9649        0.9667        0.9684        0.9632        0.9649       \n",
      "              | 0.03          4.10          3.12          4.35          0.03         \n",
      "4      100    | Simple        Fixed-LUT     Fixed-Wider   Fixed-Taylor  XGB          \n",
      "----------------------------------------------------------------------------------------------------\n",
      "              | 0.9649        0.9737        0.9719        0.9702        0.9649       \n",
      "              | 0.04          8.24          3.46          7.97          0.04         \n",
      "5      50     | Simple        Fixed-LUT     Fixed-Wider   Fixed-Taylor  XGB          \n",
      "----------------------------------------------------------------------------------------------------\n",
      "              | 0.9649        0.9667        0.9684        0.9649        0.9649       \n",
      "              | 0.02          7.10          4.23          6.64          0.02         \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Datasets:  50%|█████     | 1/2 [06:31<06:31, 391.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5      100    | Simple        Fixed-LUT     Fixed-Wider   Fixed-Taylor  XGB          \n",
      "----------------------------------------------------------------------------------------------------\n",
      "              | 0.9702        0.9649        0.9684        0.9702        0.9702       \n",
      "              | 0.04          13.37         4.52          10.90         0.04         \n",
      "\n",
      "=== Dataset: credit_card_default ===\n",
      "4      50     | Simple        Fixed-LUT     Fixed-Wider   Fixed-Taylor  XGB          \n",
      "----------------------------------------------------------------------------------------------------\n",
      "              | 0.8202        0.8198        0.8201        0.8198        0.8202       \n",
      "              | 0.53          14.98         13.74         24.67         0.54         \n",
      "4      100    | Simple        Fixed-LUT     Fixed-Wider   Fixed-Taylor  XGB          \n",
      "----------------------------------------------------------------------------------------------------\n",
      "              | 0.8187        0.8194        0.8196        0.8194        0.8187       \n",
      "              | 1.04          214.38        211.12        236.05        1.05         \n",
      "5      50     | Simple        Fixed-LUT     Fixed-Wider   Fixed-Taylor  XGB          \n",
      "----------------------------------------------------------------------------------------------------\n",
      "              | 0.8187        0.8194        0.8199        0.8199        0.8187       \n",
      "              | 0.74          213.45        217.25        214.45        0.78         \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Datasets: 100%|██████████| 2/2 [2:11:51<00:00, 3955.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5      100    | Simple        Fixed-LUT     Fixed-Wider   Fixed-Taylor  XGB          \n",
      "----------------------------------------------------------------------------------------------------\n",
      "              | 0.8175        0.8198        0.8192        0.8200        0.8175       \n",
      "              | 1.76          39.20         35.78         60.66         1.77         \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import xgboost as xgb\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "\n",
    "SCALE = 100000\n",
    "INV_SCALE = 1 / SCALE\n",
    "\n",
    "# --- Fixed-point helpers ---\n",
    "def float_to_fixed(x): return int(round(x * SCALE))\n",
    "def fixed_to_float(x): return x * INV_SCALE\n",
    "def fixed_mul(x, y): return (x * y) // SCALE\n",
    "def fixed_div(x, y): return (x * SCALE) // y if y != 0 else 0\n",
    "\n",
    "# --- Fixed sigmoid functions ---\n",
    "def lut_fixed_sigmoid(x):\n",
    "    x = max(-500000, min(500000, x))\n",
    "    key = 1000 * round(x / 1000)\n",
    "    return SIGMOID_LUT.get(key, SCALE // 2)\n",
    "\n",
    "SIGMOID_LUT = {}\n",
    "def init_sigmoid_lut():\n",
    "    for x in range(-500000, 500001, 1000):\n",
    "        xf = x / SCALE\n",
    "        SIGMOID_LUT[x] = int(round((1 / (1 + math.exp(-xf))) * SCALE))\n",
    "init_sigmoid_lut()\n",
    "\n",
    "def wider_piecewise_sigmoid(x):\n",
    "    two_scale = 2 * SCALE\n",
    "    if x <= -two_scale: return 0\n",
    "    if x >= two_scale: return SCALE\n",
    "    return (x + two_scale) // 4\n",
    "\n",
    "def taylor_fixed_sigmoid(x, terms=10):\n",
    "    x = max(-float_to_fixed(5), min(float_to_fixed(5), x))\n",
    "    exp_neg_x = SCALE\n",
    "    term = SCALE\n",
    "    for i in range(1, terms + 1):\n",
    "        term = fixed_mul(term, fixed_div(-x, float_to_fixed(i)))\n",
    "        exp_neg_x += term\n",
    "    denom = SCALE + exp_neg_x\n",
    "    return fixed_div(SCALE, denom)\n",
    "\n",
    "# --- Core fixed-point boosting model ---\n",
    "class XGBoostTreeClassifier:\n",
    "    def __init__(self, max_depth=3, lambda_=1, gamma=0, colsample_bytree=1.0, num_bins=64):\n",
    "        self.max_depth = max_depth\n",
    "        self.lambda_ = lambda_\n",
    "        self.gamma = gamma\n",
    "        self.colsample_bytree = colsample_bytree\n",
    "        self.num_bins = num_bins\n",
    "        self.tree = None\n",
    "\n",
    "    def fit(self, X, grad, hess, depth=0):\n",
    "        if depth >= self.max_depth or len(X) < 2:\n",
    "            return np.clip(np.sum(grad) / (np.sum(hess) + self.lambda_), -1, 1)\n",
    "\n",
    "        best_gain = -float('inf')\n",
    "        best_feat, best_split = None, None\n",
    "        n_features = X.shape[1]\n",
    "        features = np.random.choice(n_features, max(1, int(n_features * self.colsample_bytree)), replace=False)\n",
    "\n",
    "        for j in features:\n",
    "            x = X[:, j]\n",
    "            if np.min(x) == np.max(x): continue\n",
    "            bin_ids = np.digitize(x, np.linspace(np.min(x), np.max(x), self.num_bins + 1)[:-1])\n",
    "            G = np.bincount(bin_ids, weights=grad, minlength=self.num_bins + 1)\n",
    "            H = np.bincount(bin_ids, weights=hess, minlength=self.num_bins + 1)\n",
    "            G_L = H_L = 0.0\n",
    "            G_total, H_total = np.sum(G), np.sum(H)\n",
    "            for b in range(1, self.num_bins):\n",
    "                G_L += G[b - 1]\n",
    "                H_L += H[b - 1]\n",
    "                G_R = G_total - G_L\n",
    "                H_R = H_total - H_L\n",
    "                if H_L == 0 or H_R == 0: continue\n",
    "                gain = 0.5 * (G_L**2 / (H_L+self.lambda_) + G_R**2 / (H_R+self.lambda_)) - 0.5 * (G_total**2 / (H_total+self.lambda_))\n",
    "                if gain > best_gain and gain > self.gamma:\n",
    "                    best_gain = gain\n",
    "                    best_feat, best_split = j, b\n",
    "\n",
    "        if best_feat is None:\n",
    "            return np.clip(np.sum(grad) / (np.sum(hess) + self.lambda_), -1, 1)\n",
    "\n",
    "        bin_edges = np.linspace(np.min(X[:, best_feat]), np.max(X[:, best_feat]), self.num_bins + 1)\n",
    "        split_val = bin_edges[best_split]\n",
    "        left = X[:, best_feat] <= split_val\n",
    "        right = ~left\n",
    "        left_tree = self.fit(X[left], grad[left], hess[left], depth + 1)\n",
    "        right_tree = self.fit(X[right], grad[right], hess[right], depth + 1)\n",
    "        return (best_feat, split_val, left_tree, right_tree)\n",
    "\n",
    "    def predict_row(self, x, node):\n",
    "        if not isinstance(node, tuple): return node\n",
    "        feat, split, l, r = node\n",
    "        return self.predict_row(x, l) if x[feat] <= split else self.predict_row(x, r)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.array([self.predict_row(x, self.tree) for x in X])\n",
    "\n",
    "class GenericFixedPointXGB:\n",
    "    def __init__(self, sigmoid_fn, n_estimators=50, max_depth=3, learning_rate=0.1, lambda_=1, gamma=0):\n",
    "        self.sigmoid_fn = sigmoid_fn\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_depth = max_depth\n",
    "        self.learning_rate = learning_rate\n",
    "        self.lambda_ = lambda_\n",
    "        self.gamma = gamma\n",
    "        self.scaled_lr = float_to_fixed(learning_rate)\n",
    "        self.trees = []\n",
    "        self.initial_logit = 0\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        y_fixed = np.array([float_to_fixed(yi) for yi in y], dtype=int)\n",
    "        y_mean = np.mean(y_fixed)\n",
    "        p = y_mean / SCALE\n",
    "        self.initial_logit = float_to_fixed(np.log(p / (1 - p)))\n",
    "        y_pred = np.full_like(y_fixed, self.initial_logit)\n",
    "\n",
    "        for _ in range(self.n_estimators):\n",
    "            p_pred = np.array([self.sigmoid_fn(val) for val in y_pred], dtype=int)\n",
    "            grad = p_pred - y_fixed\n",
    "            hess = (p_pred * (SCALE - p_pred)) // SCALE\n",
    "            tree = XGBoostTreeClassifier(max_depth=self.max_depth, lambda_=self.lambda_, gamma=self.gamma)\n",
    "            tree.tree = tree.fit(X, grad, hess)\n",
    "            update = tree.predict(X)\n",
    "            for i in range(len(y_pred)):\n",
    "                y_pred[i] -= fixed_mul(self.scaled_lr, float_to_fixed(update[i]))\n",
    "            self.trees.append(tree)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        y_pred = np.full(X.shape[0], self.initial_logit)\n",
    "        for tree in self.trees:\n",
    "            update = tree.predict(X)\n",
    "            for i in range(len(y_pred)):\n",
    "                y_pred[i] -= fixed_mul(self.scaled_lr, float_to_fixed(update[i]))\n",
    "        return np.array([self.sigmoid_fn(val) for val in y_pred], dtype=int)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return (self.predict_proba(X) >= SCALE // 2).astype(int)\n",
    "\n",
    "# --- Classifiers ---\n",
    "FixedLUT = lambda **kw: GenericFixedPointXGB(lut_fixed_sigmoid, **kw)\n",
    "FixedWider = lambda **kw: GenericFixedPointXGB(wider_piecewise_sigmoid, **kw)\n",
    "FixedTaylor = lambda **kw: GenericFixedPointXGB(taylor_fixed_sigmoid, **kw)\n",
    "\n",
    "def load_credit_dataset():\n",
    "    data = np.genfromtxt('credit_default.csv', delimiter=',', skip_header=1, filling_values=0)\n",
    "    X_credit = data[:, 1:-1]  # skip ID column and select features\n",
    "    y_credit = (data[:, -1] > 0.5).astype(int)  # last column is the target\n",
    "    return X_credit, y_credit\n",
    "\n",
    "# --- Benchmark ---\n",
    "def run_benchmark():\n",
    "    datasets = {\n",
    "        \"breast_cancer\": load_breast_cancer(return_X_y=True),\n",
    "        \"credit_card_default\": load_credit_dataset(),\n",
    "    }\n",
    "    depths = [4,5]\n",
    "    trees_list = [50,100]\n",
    "    num_runs = 5\n",
    "\n",
    "    for name, (X, y) in tqdm(datasets.items(), desc=\"Datasets\"):\n",
    "        print(f\"\\n=== Dataset: {name} ===\")\n",
    "        X = StandardScaler().fit_transform(X)\n",
    "\n",
    "        for depth in depths:\n",
    "            for trees in trees_list:\n",
    "                accs = {\"Simple\": [], \"Fixed-LUT\": [], \"Fixed-Wider\": [], \"Fixed-Taylor\": [], \"XGB\": []}\n",
    "                times = {k: [] for k in accs}\n",
    "\n",
    "                for run in range(num_runs):\n",
    "                    Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=0.2, random_state=42+run)\n",
    "\n",
    "                    models = {\n",
    "                        \"Simple\": xgb.XGBClassifier(n_estimators=trees, max_depth=depth, verbosity=0, eval_metric=\"logloss\"),\n",
    "                        \"Fixed-LUT\": FixedLUT(n_estimators=trees, max_depth=depth),\n",
    "                        \"Fixed-Wider\": FixedWider(n_estimators=trees, max_depth=depth),\n",
    "                        \"Fixed-Taylor\": FixedTaylor(n_estimators=trees, max_depth=depth),\n",
    "                        \"XGB\": xgb.XGBClassifier(n_estimators=trees, max_depth=depth, verbosity=0, eval_metric=\"logloss\"),\n",
    "                    }\n",
    "\n",
    "                    for label, model in models.items():\n",
    "                        t0 = time.time()\n",
    "                        model.fit(Xtr, ytr)\n",
    "                        acc = np.mean(model.predict(Xte) == yte)\n",
    "                        accs[label].append(acc)\n",
    "                        times[label].append(time.time() - t0)\n",
    "\n",
    "                print(f\"{depth:<6} {trees:<6} | \" + \" \".join(f\"{label:<13}\" for label in accs))\n",
    "                print(\"-\" * 100)\n",
    "                print(\" \" * 13 + \" | \" + \" \".join(f\"{np.mean(accs[k]):<13.4f}\" for k in accs))\n",
    "                print(\" \" * 13 + \" | \" + \" \".join(f\"{np.mean(times[k]):<13.2f}\" for k in times))\n",
    "\n",
    "run_benchmark()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1017edb1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
